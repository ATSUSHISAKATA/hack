{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16512 train + 4128 test\n",
      "Index(['<1H OCEAN', 'NEAR OCEAN', 'INLAND', 'NEAR BAY', 'ISLAND'], dtype='object')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'FeatureUnion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5b24b0c6aa72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m full_pipeline = FeatureUnion(transformer_list=[\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'num_pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_pipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'cat_pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcat_pipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FeatureUnion' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH= os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL= DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "\n",
    "#カリフォルニア住宅データ\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path=os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz=tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "fetch_housing_data()    \n",
    "    \n",
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "        csv_path = os.path.join(housing_path,\"housing.csv\")\n",
    "        return pd.read_csv(csv_path)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "housing = load_housing_data()\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#housing.hist(bins=50,figsize=(20,15))\n",
    "#plt.show()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def split_train_test(data,test_radio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data)*test_radio)\n",
    "    test_indices=shuffled_indices[:test_set_size]\n",
    "    train_indices=shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices],data.iloc[test_indices]\n",
    "\n",
    "train_set, test_set = split_train_test(housing, 0.2)\n",
    "print(len(train_set), \"train +\" ,len(test_set), \"test\")\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def test_set_check(identifier, test_ratio, hash):\n",
    "    return hash(np.int64(identifier)).digest()[-1] <256 * test_ratio\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column, hash=hashlib.md5):\n",
    "    ids=data[id_column]\n",
    "    in_test_set=ids.apply(lambda id_: test_set_check(id_, test_ratio, hash))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]\n",
    "\n",
    "housing_with_id=housing.reset_index()\n",
    "housing_with_id[\"id\"]=housing[\"longitude\"]*1000 +housing[\"latitude\"]\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id,0.2,\"id\")\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state =42)\n",
    "\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"]/1.5)\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5,5.0, inplace=True)\n",
    "\n",
    "#層化抽出法\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "    \n",
    "\n",
    "strat_test_set[\"income_cat\"].value_counts()/len(strat_test_set)    \n",
    "\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\",axis=1, inplace=True)\n",
    "    \n",
    "housing=strat_train_set.copy()\n",
    "\n",
    "#housing.plot(kind=\"scatter\",x=\"longitude\", y=\"latitude\")\n",
    "#housing.plot(kind=\"scatter\",x=\"longitude\", y=\"latitude\",alpha=0.1)\n",
    "#housing.plot(kind=\"scatter\",x=\"longitude\", y=\"latitude\",alpha=0.1,s=housing[\"population\"]/100,label=\"population\", figsize=(10,7),\n",
    "#            c=\"median_house_value\",cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    "#            )\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "corr_matrix=housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes=[\"median_house_value\",\"median_income\",\"total_rooms\",\"housing_median_age\"]\n",
    "#scatter_matrix(housing[attributes],figsize=(12,8))\n",
    "\n",
    "#housing.plot(kind=\"scatter\",x=\"median_income\",y=\"median_house_value\",alpha=0.1)\n",
    "\n",
    "housing[\"rooms_per_household\"]=housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_rooms\"]=housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"Population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n",
    "#corr_matrix=housing.corr()\n",
    "#corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
    "\n",
    "housing = strat_train_set.drop(\"median_house_value\",axis=1)\n",
    "housing_labels=strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "#housing.dropna(subset=[\"total_bedrooms\"])\n",
    "#housing.drop(\"total_bedrooms\",axis=1)\n",
    "#median=housing[\"total_bedrooms\"].median()\n",
    "#housing[\"total_bedrooms\"].fillna(median,inplace=True)\n",
    "\n",
    "#欠損値補完\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer=SimpleImputer(strategy=\"median\")\n",
    "\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "imputer.fit(housing_num)\n",
    "imputer.statistics_\n",
    "housing_num.median().values\n",
    "\n",
    "X=imputer.transform(housing_num)\n",
    "housing_tr=pd.DataFrame(X, columns=housing_num.columns)\n",
    "\n",
    "housing_cat=housing[\"ocean_proximity\"]\n",
    "housing_cat.head(10)\n",
    "housing_cat_encoded, housing_categories = housing_cat.factorize()\n",
    "housing_cat_encoded[:10]\n",
    "print(housing_categories)\n",
    "\n",
    "#カテゴリ1/0処理\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder=OneHotEncoder(categories='auto')\n",
    "housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\n",
    "#print(housing_cat_1hot)\n",
    "#housing_cat_1hot.toarray()\n",
    "#from sklearn.preprocessing import CategoricalEncoder\n",
    "#cat_encoder = CategoricalEncoder()\n",
    "#housing_cat_reshaped = housing_cat.values.reshape(-1,1)\n",
    "#housing_cat_1hot = cat_encoder.fit_transform(housing_cat_reshped)\n",
    "#housing_cat_1hot\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix=3,4,5,6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room=True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X,y=None):\n",
    "        rooms_per_household = X[:, rooms_ix]/X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix]/X[:,household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room=X[:,bedrooms_ix]/X[:,rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]##c_列ベクトル結合\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "        \n",
    "        \n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "\n",
    "#パイプライン通す\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\",imputer),\n",
    "    (\"attribs_adder\",CombinedAttributesAdder()),\n",
    "    (\"std_sclaer\", StandardScaler()),\n",
    "])\n",
    "        \n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "#属性選択クラス\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        return X[self.attribute_names].values\n",
    "    \n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs=[\"ocean_proximity\"]\n",
    "    \n",
    "num_pipeline = Pipeline([\n",
    "    (\"selector\", DataFrameSelector(num_attribs)),\n",
    "    (\"imputer\", imputer),\n",
    "    (\"attribs_adder\", CombinedAttributesAdder()),\n",
    "    (\"std_scaler\", StandardScaler()),\n",
    "])    \n",
    "\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "('selector', DataFrameSelector(cat_attribs)),\n",
    "('encoder', OneHotEncoder(sparse=False,categories='auto'))##categories='auto',\n",
    "])\n",
    "\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "('num_pipeline',num_pipeline),\n",
    "('cat_pipeline',cat_pipeline),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#try:\n",
    "##    from sklearn.compose import ColumnTransformer\n",
    "#except ImportError:\n",
    "#    from future_encoders import ColumnTransformer\n",
    "#num_attribs = list(housing_num)\n",
    "#cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "#full_pipeline = ColumnTransformer([\n",
    "#        (\"num\", num_pipeline, num_attribs),\n",
    "#        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "#    ])\n",
    "\n",
    "housing_prepared=full_pipeline.fit_transform(housing)\n",
    "print(type(housing_prepared[1]))\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "some_data_prepared = num_pipeline.fit_transform(some_data)\n",
    "#print(\"Predictions:\", lin_reg.predict(some_data_prepared))\n",
    "#print(\"Labels:\", list(some_labels))\n",
    "\n",
    "#MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse=mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse=np.sqrt(lin_mse)\n",
    "#lin_rmse\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "#tree_rmse\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores=cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\",scores.std())\n",
    "    \n",
    "#display_scores(tree_rmse_scores)    \n",
    "\n",
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,scoring=\"neg_mean_squared_error\",cv=10)\n",
    "lin_rmse_scores=np.sqrt(-lin_scores)\n",
    "#display_scores(lin_rmse_scores)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "forest_reg_predictions = forest_reg.predict(housing_prepared)\n",
    "forest_reg_mse = mean_squared_error(housing_labels, forest_reg_predictions)\n",
    "forest_rmse = np.sqrt(forest_reg_mse)\n",
    "forest_rmse\n",
    "forest_reg_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,scoring=\"neg_mean_squared_error\",cv=10)\n",
    "forest_reg_rmse_scores=np.sqrt(-forest_reg_scores)\n",
    "#display_scores(forest_reg_rmse_scores)\n",
    "#from sklearn.externals import joblib\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid=[\n",
    "    {\"n_estimators\":[3,10,30],\"max_features\":[2,4,6,8]},\n",
    "    {\"bootstrap\":[False],\"n_estimators\":[3,10],\"max_features\":[2,3,4]},\n",
    "]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                          scoring=\"neg_mean_squared_error\",refit=True,\n",
    "                        )\n",
    "\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "grid_search.best_params_\n",
    "grid_search.best_estimator_\n",
    "\n",
    "cvres= grid_search.cv_results_\n",
    "#for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "#    print(np.sqrt(-mean_score),params)\n",
    "    \n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "feature_importances\n",
    "\n",
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\",\"bedrooms_per_room\"]\n",
    "attribs=num_attribs + extra_attribs\n",
    "sorted(zip(feature_importances,attributes),reverse=True)\n",
    "\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"median_house_value\",axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "    \n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse=mean_squared_error(y_test,final_predictions)\n",
    "final_rmse=np.sqrt(final_mse)\n",
    "\n",
    "\n",
    "#SVM\n",
    "from sklearn.svm import SVR #回帰の場合はSVCではなくてSVRを使う\n",
    "\n",
    "param_grid = [\n",
    "        {'kernel': ['linear'], 'C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.0]},\n",
    "        {'kernel': ['rbf'], 'C': [1.0, 3.0, 10., 30., 100., 300., 1000.0],\n",
    "         'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},\n",
    "    ]\n",
    "svm_reg = SVR()   #モデルのパラメータはすべて辞書形式でまとめる　rbf_SVC=SVC(kernel=\"linear\")　こうしない\n",
    "grid_search = GridSearchCV(svm_reg, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2, n_jobs=4)\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "cvres=grid_search.cv_results_\n",
    "negative_mse=cvres[\"mean_test_score\"]\n",
    "#negative_mse = grid_search.best_score_\n",
    "rmse = np.sqrt(-negative_mse)\n",
    "rmse\n",
    "\n",
    "grid_search.best_params_\n",
    "\n",
    "##設問２\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import expon, reciprocal\n",
    "param_distribs = {\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'C': reciprocal(20, 200000),\n",
    "        'gamma': expon(scale=1.0),\n",
    "    }\n",
    "\n",
    "svm_reg = SVR()\n",
    "rnd_search = RandomizedSearchCV(svm_reg, param_distributions=param_distribs,\n",
    "                                n_iter=50, cv=5, scoring='neg_mean_squared_error',\n",
    "                                verbose=2, n_jobs=4, random_state=42)\n",
    "rnd_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "\n",
    "feature_importancesa = rnd_search.best_estimator_.feature_importances_\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "def indices_of_top_k(arr,k):\n",
    "    return np,sort(np.argpartition(np.array(arr), -k)[-k:])\n",
    "\n",
    "#重要な特徴量抽出\n",
    "class TopFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def _init_(seld,feature_importances,k):\n",
    "        self.feature_importances=feature_importances\n",
    "        self.k=k\n",
    "    def fit(self, X,y=None):\n",
    "        self.feature_indices_=indices_of_top_k(self.feature_importances, self,k)\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        return X[:, self.feature_indices]\n",
    "    \n",
    "k = 5\n",
    "\n",
    "prepare_Select_And_predict_pipeline=Pipeline([\n",
    "    (\"preparation\", full_pipeline),\n",
    "    (\"feature_selection\",TopFeatureSelector(feature_importances,k)),\n",
    "    (\"svm_reg\",SVR(**rnd_search.best_params_))  \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#前処理のオプション\n",
    "param_grid=[{\n",
    "    \"preparation__num__imputer__strategy\":[\"mean\",\"median\",\n",
    "                                       \"most_frequent\"],\n",
    "    \"feature_selection__k\": list(range(1, len(feature_imporances)+1))\n",
    "}]\n",
    "\n",
    "grid_search_prep = GridSearchCV(prepare_select_and_predict_pipeline,param_grid,cv=5,\n",
    "                                scoring=\"neg_mean_squared_error\",verbose=2,n_jobs=4)\n",
    "\n",
    "grid_search_prep.fit(housing, housing_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "        {'kernel': ['linear'], 'C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.0]},\n",
    "        {'kernel': ['rbf'], 'C': [1.0, 3.0, 10., 30., 100., 300., 1000.0],\n",
    "         'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},\n",
    "    ]\n",
    "svm_reg = SVR()   #モデルのパラメータはすべて辞書形式でまとめる　rbf_SVC=SVC(kernel=\"linear\")　こうしない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH= os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL= DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL,housing_path=HOUSING_PATH):\n",
    "    os.makedirs(HOUSING_PATH,exist_ok=True)\n",
    "    tgz_path=os.path.join(housing_path,\"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url,tgz_path)\n",
    "    housing_tgz=tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "    \n",
    "    \n",
    "fetch_housing_data()\n",
    "\n",
    "import pandas as pd\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path=os.path.join(housing_path,\"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "    \n",
    "housing=load_housing_data()\n",
    "housing.head()\n",
    "\n",
    "housing.info()\n",
    "\n",
    "housing[\"ocean_proximity\"].value_counts()\n",
    "housing.describe()\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50,figsize=(20,15))\n",
    "import numpy as np\n",
    "\n",
    "def split_train_test(data,test_ratio):\n",
    "    shuffled_index=np.random.permutation(len(data))\n",
    "    test_size=int(test_ratio*len(data))\n",
    "    test_indices=shuffled_index[:test_size]\n",
    "    train_indices=shuffled_index[test_size:]\n",
    "    return data.iloc[train_indices],data.iloc[test_indices]\n",
    "\n",
    "train_set,test_set=split_train_test(housing,0.2)\n",
    "len(train_set),len(test_set)\n",
    "from zlib import crc32\n",
    "\n",
    "def test_set_check(identifier,test_ratio):\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff <test_ratio *2**32\n",
    "\n",
    "def split_train_test_by_id(data,test_ratio,id_column):\n",
    "    ids=data[id_column]\n",
    "    in_test_set=ids.apply(lambda id_:test_set_check(id_,test_ratio))\n",
    "    return data.loc[~in_test_set],data.loc[in_test_set]\n",
    "\n",
    "housing_with_id=housing.reset_index()\n",
    "train_set,test_set=split_train_test_by_id(housing_with_id,0.2,\"index\")\n",
    "\n",
    "housing_with_id[\"id\"]=housing[\"longitude\"]*1000+housing[\"latitude\"]\n",
    "train_set,test_set=split_train_test_by_id(housing_with_id,0.2,\"id\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set,test_set=train_test_split(housing,test_size=0.2,random_state=42)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "housing[\"income_cat\"]=pd.cut(housing[\"median_income\"],bins=[0.,1.5,3,4.5,6.,np.inf],labels=[1,2,3,4,5])\n",
    "housing[\"income_cat\"].hist()\n",
    "\n",
    "#層化抽出\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\n",
    "for train_index,test_index in split.split(housing,housing[\"income_cat\"]):\n",
    "    strat_train_set=housing.loc[train_index]\n",
    "    strat_test_set=housing.loc[test_index]\n",
    "    \n",
    "#strat_test_set[\"income_cat\"].value_counts(normalize=True)\n",
    "strat_test_set[\"income_cat\"].value_counts()/len(strat_test_set)\n",
    "\n",
    "for set_ in (strat_train_set,strat_test_set): \n",
    "    set_.drop(\"income_cat\",axis=1,inplace=True)#inplaceで元のオブジェクト交換\n",
    "    \n",
    "housing=strat_train_set.copy()\n",
    "\n",
    "#housing.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\",alpha=0.1)\n",
    "#housing.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\",alpha=0.4,s=housing[\"population\"]/100,label=\"population\",figsize=(10,7),c=\"median_house_value\",cmap=plt.get_cmap(\"jet\"),colorbar=True,)\n",
    "#plt.legend()\n",
    "\n",
    "corr_matrix=housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "attribes=[\"median_house_value\",\"median_income\",\"total_rooms\",\"housing_median_age\"]\n",
    "scatter_matrix(housing[attribes],figsize=(12,8))\n",
    "#housing.plot(kind=\"scatter\",x=\"median_income\",y=\"median_house_value\",alpha=0.1)\n",
    "\n",
    "housing[\"rooms_per_household\"]=housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"]=housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n",
    "\n",
    "corr_matrix=housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
    "\n",
    "housing=strat_train_set.drop(\"median_house_value\",axis=1)\n",
    "housing_labels=strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "housing.drop(\"total_bedrooms\",axis=1)\n",
    "housing.dropna(subset=[\"total_bedrooms\"])\n",
    "median=housing[\"total_bedrooms\"].median()\n",
    "housing[\"total_bedrooms\"].fillna(median,inplace=True)\n",
    "\n",
    "#欠損地補完\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer=SimpleImputer(strategy=\"median\")\n",
    "housing_num=housing.drop(\"ocean_proximity\",axis=1)\n",
    "imputer.fit(housing_num)\n",
    "imputer.statistics_\n",
    "housing_num.median().values\n",
    "\n",
    "X=imputer.transform(housing_num)\n",
    "\n",
    "housing_tr=pd.DataFrame(X,columns=housing_num.columns,index=housing_num.index)\n",
    "#インスタンス変数　https://uxmilk.jp/41600\n",
    "#オブジェクト　すべての総称　クラス　設計書　インスタンス　クラスからインスタンス化されたオブジェクト\n",
    "#コンストラクタ　クラスをインスタンス化する際に実行される特別なメソッド。\n",
    "\n",
    "housing_cat=housing[[\"ocean_proximity\"]]#[[二つで1次元が2次元表記\n",
    "housing_cat.head(10)\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "encoder=OrdinalEncoder()\n",
    "housing_cat_encoded=encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10]\n",
    "encoder.categories_\n",
    "\n",
    "#カテゴリー処理\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder=OneHotEncoder()#sparse=False)\n",
    "housing_cat_1hot=cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot\n",
    "cat_encoder.categories_\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_ix,bedrooms_ix,population_ix,households_ix=3,4,5,6\n",
    "\n",
    "#属性変換\n",
    "class CombinedAttributesAdder(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,add_bedrooms_per_room=True):\n",
    "        self.add_bedrooms_per_room=add_bedrooms_per_room\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        rooms_per_household = X[:,rooms_ix]/X[:,households_ix]\n",
    "        population_per_households=X[:,population_ix]/X[:,households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room=X[:,bedrooms_ix]/X[:,rooms_ix]\n",
    "            return np.c_[X,rooms_per_household,population_per_households,bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X,rooms_per_household,population_per_households]\n",
    "        \n",
    "attr_adder=CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs=attr_adder.transform(housing.values)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline=Pipeline([\n",
    "    (\"imputer\",SimpleImputer(strategy=\"median\")),\n",
    "    (\"attr_adder\",CombinedAttributesAdder()),\n",
    "    (\"std_scaler\",StandardScaler())\n",
    "])\n",
    "\n",
    "housing_num_tr=num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "full_pipeline=ColumnTransformer([\n",
    "    (\"num\",num_pipeline,num_attribs),\n",
    "    (\"cat\",OneHotEncoder(),cat_attribs),\n",
    "])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg=LinearRegression()\n",
    "lin_reg.fit(housing_prepared,housing_labels)\n",
    "\n",
    "some_data=housing.iloc[:5]\n",
    "some_labels=housing_labels[:5]\n",
    "some_data_prepared=full_pipeline.transform(some_data)\n",
    "#print(some_data_prepared==housing_prepared[:5])\n",
    "print(\"predictions :\",lin_reg.predict(some_data_prepared))\n",
    "print(\"labels :\", list(some_labels))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions=lin_reg.predict(housing_prepared)\n",
    "lin_mse=mean_squared_error(housing_labels,housing_predictions)\n",
    "lin_rmse=np.sqrt(lin_mse)\n",
    "print(\"RMSE :\",lin_rmse)\n",
    "\n",
    "#決定木\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared,housing_labels)\n",
    "\n",
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse=mean_squared_error(housing_labels,housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse\n",
    "\n",
    "#交差検証\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores=cross_val_score(tree_reg,housing_prepared,housing_labels,scoring = \"neg_mean_squared_error\",cv=10)\n",
    "tree_rmse_scores=np.sqrt(-scores)\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "    \n",
    "display_scores(tree_rmse_scores) \n",
    "lin_scores = cross_val_score(lin_reg,housing_prepared,housing_labels,scoring = \"neg_mean_squared_error\",cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)\n",
    "\n",
    "#ランダムフォレスト\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "forest_scores = cross_val_score(forest_reg,housing_prepared,housing_labels,scoring = \"neg_mean_squared_error\",cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)\n",
    "\n",
    "#モデル保存\n",
    "import joblib\n",
    "joblib.dump(tree_rmse_scores, r\"C:\\Users\\a.sakata\\py3env\\my_env\\machine_learning\\2sec_model\\tree_reg.pkl\")\n",
    "joblib.dump(forest_rmse_scores, r\"C:\\Users\\a.sakata\\py3env\\my_env\\machine_learning\\2sec_model\\forest_reg.pkl\")\n",
    "joblib.dump(lin_rmse_scores, r\"C:\\Users\\a.sakata\\py3env\\my_env\\machine_learning\\2sec_model\\lin_reg.pkl\")\n",
    "\n",
    "my_model_loaded=joblib.load(r\"C:\\Users\\a.sakata\\py3env\\my_env\\machine_learning\\2sec_model\\lin_reg.pkl\")\n",
    "\n",
    "#グリッドサーチ、交差検証\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_dict=[\n",
    "    {\"n_estimators\":[3,10,30],\"max_features\":[2,4,6,8]},\n",
    "    {\"bootstrap\":[False],\"n_estimators\":[3,10],\"max_features\":[2,3,4]},\n",
    "]\n",
    "\n",
    "forest_reg=RandomForestRegressor()\n",
    "grid=GridSearchCV(forest_reg,param_dict,cv=5,scoring=\"neg_mean_squared_error\",return_train_score=True)\n",
    "\n",
    "grid.fit(housing_prepared,housing_labels)\n",
    "grid.best_params_\n",
    "grid.best_estimator_\n",
    "joblib.dump(r\"C:\\Users\\a.sakata\\py3env\\my_env\\machine_learning\\2sec_model\\grid_forest_reg.pkl\")\n",
    "cvrs=grid.cv_results_\n",
    "for mean_score,params in zip(cvrs[\"mean_test_score\"],cvrs[\"params\"]):\n",
    "    print(np.sqrt(mean_score),params)\n",
    "joblib.dump(grid,r\"C:\\Users\\a.sakata\\py3env\\my_env\\machine_learning\\2sec_model\\grid_forest_reg.pkl\")   \n",
    "\n",
    "feature_importances=grid.best_estimator_.feature_importances_\n",
    "feature_importances\n",
    "\n",
    "extra_attribs=[\"rooms_per_household\",\"population_per_households\",\"bedrooms_per_room\"]\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes=num_attribs+extra_attribs+cat_one_hot_attribs\n",
    "sorted(zip(attributes,feature_importances),reverse = True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
