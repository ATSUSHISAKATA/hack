{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................. , score=0.98125, total=   0.2s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................. , score=0.98125, total=   0.0s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................... , score=0.9925, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 93.94%\n",
      "Recall: 97.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.sakata\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#spamデータ分類\n",
    "import os\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_ROOT=\"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "HAM_URL=DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
    "SPAM_URL=DOWNLOAD_ROOT+ \"20030228_spam.tar.bz2\"\n",
    "SPAM_PATH = os.path.join(\"datasets\",\"spam\")\n",
    "\n",
    "def fetch_spam_data(spam_url=SPAM_URL,spam_path=SPAM_PATH):\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "    for filename, url in ((\"ham.tar.bz2\"),HAM_URL),(\"spam.tar.bz2\", SPAM_URL):#()分割\n",
    "        path=os.path.join(spam_path,filename)\n",
    "        if not os.path.isfile(path):        \n",
    "            urllib.request.urlretrieve(url,path)#url指定、保存先の名前指定\n",
    "        tar_bz2_file=tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=SPAM_PATH)\n",
    "        tar_bz2_file.close()\n",
    "        \n",
    "fetch_spam_data()#これ呼び出してファイル取得\n",
    "\n",
    "HAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\n",
    "SPAM_DIR= os.path.join(SPAM_PATH,\"spam\")\n",
    "ham_filenames=[name for name in sorted(os.listdir(HAM_DIR))\n",
    "              if len(name)>20] #名前の長さが20以上のファイルを抽出\n",
    "spam_filenames=[name for name in sorted(os.listdir(SPAM_DIR))\n",
    "               if len(name)>20]\n",
    "import email\n",
    "import email.policy\n",
    "\n",
    "#電子メールのメッセージヘッダのためのインターネット規定のフォーマット.RFC　822をパースする　emailモジュールを用いる\n",
    "def load_email(is_spam, filename, spam_path=SPAM_PATH):\n",
    "    directory=\"spam\" if is_spam else \"easy_ham\"\n",
    "    with open(os.path.join(spam_path, directory, filename),\"rb\")as f: ##with open ~:で使う\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
    "    \n",
    "ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]    \n",
    "spam_emails=[load_email(is_spam=True, filename=name) for name in spam_filenames]\n",
    "\n",
    "#print(ham_emails[1].get_content().strip())#テキストファイル読み込み、strip()は両端の空白、改行を取り除く\n",
    "#print(spam_emails[6].get_content().strip())\n",
    "\n",
    "def get_email_structure(email):\n",
    "    if isinstance(email,str):#型判定\n",
    "        return email\n",
    "    payload=email.get_payload()#現在のペイロードへの参照を返す。\n",
    "    if isinstance(payload, list):\n",
    "        return \"multipart({})\".format(\", \".join([\n",
    "            get_email_structure(sub_email)\n",
    "            for sub_email in payload\n",
    "                    ]))\n",
    "    else:\n",
    "        return email.get_content_type()#Return the message’s content type, coerced to lower case of the form maintype/subtype.\n",
    "    \n",
    "from collections import Counter\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures=Counter()\n",
    "    for email in emails:\n",
    "        structure=get_email_structure(email)\n",
    "        structures[structure]+=1\n",
    "    return structures\n",
    "\n",
    "structures_counter(ham_emails).most_common()\n",
    "structures_counter(spam_emails).most_common()\n",
    "\n",
    "#for header, value in spam_emails[0].items():\n",
    "    #print(header,\":\",value )\n",
    "    \n",
    "spam_emails[0][\"Subject\"]\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split #シャッフル機能つき　デフォルトTrue\n",
    "X=np.array(ham_emails+spam_emails)\n",
    "y=np.array([0]*len(ham_emails)+[1]*len(spam_emails))\n",
    "X_train,  X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42) #分類の分割では、この順番に出力\n",
    "\n",
    "  \n",
    "import re\n",
    "from html import unescape\n",
    "\n",
    "def html_to_plain_text(html): #*?直前の文字が₀回以上繰り返す　最短一致\n",
    "    text=re.sub(\"<head.*?>.*?</head>\", \"\",str(html), flags=re.M | re.S |re.I)#re.A (ASCII 限定マッチング)、 re.I (大文字・小文字を区別しない)、\n",
    "    text=re.sub(\"<a\\s.*?>\",\"HYPERRINK\",text, flags=re.M | re.S | re.I)                    #re.L (ロケール依存)、 re.M (複数行)、 re.S (ドットが全てにマッチ)、 re.U (Unicode マッチング)、 re.X (冗長)\n",
    "    text=re.sub(\"<.*?>\",\"\",text,flags=re.M | re.S)\n",
    "    text=re.sub(r\"(\\s*\\n)+\", \"\\n\", text, flags=re.M | re.S)\n",
    "    return unescape(text)\n",
    "\n",
    "    \n",
    "html_spam_emails = [email for email in X_train[y_train ==1 ] if get_email_structure(email)==\"text/html\"]\n",
    "sample_html_spam=html_spam_emails[7]\n",
    "#print(sample_html_spam.get_content().strip()[:1000], \"...\")\n",
    "#email　html形式をテキスト形式に変更した\n",
    "\n",
    "def email_to_text(email):\n",
    "    html=None\n",
    "    for part in email.walk():\n",
    "        ctype=part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue #上記に対応するものはスキップする\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except: \n",
    "            content=str(part.get_payload())\n",
    "        if ctype ==\"text/plain\":\n",
    "            return content\n",
    "        else:\n",
    "            html=content\n",
    "    if html:\n",
    "        return html_to_plain_text(html)\n",
    "    html_to_plain_text(email)\n",
    "    \n",
    "try:\n",
    "    import urlextract\n",
    "    \n",
    "    url_extractor=urlextract.URLExtract()\n",
    "    urls=extractor.find_urls(\"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s\")\n",
    "except ImportError:\n",
    "    print(\"error\")    \n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "       \n",
    "    stemmer=nltk.stem.PorterStemmer()   \n",
    "except ImportError:\n",
    "    print(\"error\")\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class EmailToWordCounterTransformer(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, strip_headers=True, lower_case=True,remove_punctuation=True,replace_urls=True,replace_numbers=True,stemming=True):\n",
    "        self.strip_headers=strip_headers\n",
    "        self.lower_case=lower_case\n",
    "        self.remove_punctuation=remove_punctuation\n",
    "        self.replace_urls=replace_urls\n",
    "        self.replace_numbers=replace_numbers\n",
    "        self.stemming=stemming\n",
    "    def fit(self, X,y=None):\n",
    "        return self\n",
    "    def transform(self,X,y=None):\n",
    "        X_transformed=[]\n",
    "        for email in X:\n",
    "            text = email_to_text(email) or \"\"\n",
    "            if self.lower_case:\n",
    "                text=text.lower()\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                extractor=urlextract.URLExtract()\n",
    "                urls=list(set(extractor.find_urls(text)))##重複をキャンセル\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text=text.replace(url,\" URL \")\n",
    "            if self.replace_numbers:\n",
    "                text=re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?',\"NUMBER\",text)\n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+',' ',text,flags=re.M) #\\Wで\\w単語文字列の反対検索、　re.Mで複数行検索\n",
    "            word_counts=Counter(text.split()) #前後カット\n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts=Counter(text.split())#countライブラリ\n",
    "                for word,count in word_counts.items():\n",
    "                    stemmed_word=stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word]+=count\n",
    "                word_counts=stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)  #numpyに直す\n",
    "\n",
    "X_few=X_train[:3]\n",
    "X_few_wordcounts=EmailToWordCounterTransformer().fit_transform(X_few)\n",
    "#print(X_few_wordcounts)    \n",
    "\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,vocabulary_size=1000):\n",
    "        self.vocabulary_size=vocabulary_size\n",
    "    def fit(self, X,y=None):\n",
    "        total_count=Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word]+=min(count,10)\n",
    "        most_common=total_count.most_common()[:self.vocabulary_size]\n",
    "        self.most_common_=most_common\n",
    "        self.vocabulary_ = {word: index+1 for index, (word,count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X,y=None):\n",
    "        rows=[]\n",
    "        cols=[]\n",
    "        data=[]\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word,0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data,(rows, cols)), shape=(len(X), self.vocabulary_size+1)) \n",
    "vocab_transformer=WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "X_few_vectors=vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocess_pipeline=Pipeline([\n",
    "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "])\n",
    "\n",
    "X_train_transformer = preprocess_pipeline.fit_transform(X_train)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "logreg=LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "logistic=cross_val_score(logreg,X_train_transformer,y_train,cv=3,verbose=3)\n",
    "logistic.mean()\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "X_test_transformer=preprocess_pipeline.transform(X_test)\n",
    "log_clf=LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "log_clf.fit(X_train_transformer,y_train)\n",
    "log_pred=log_clf.predict(X_test_transformer)##トレーニングデータで訓練したモデル（前処理）を用いてテストデータの前処理を行う。\n",
    "print(\"Precision: {:.2f}%\".format(100*precision_score(y_test,log_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100*recall_score(y_test,log_pred)))\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "                    \n",
    "                \n",
    "                \n",
    " \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "df_box=[]\n",
    "for i in range(len(X_train)):\n",
    "    words=X_train[i]\n",
    "    word_count=defaultdict(int)\n",
    "    for word in words.values():\n",
    "        for word in word.split():\n",
    "            word_count[word]+=1    \n",
    "    df=pd.DataFrame.from_dict(word_count, orient=\"index\").T\n",
    "    df.index=[i]\n",
    "    df_box+=df\n",
    "\n",
    "for df in df_box:\n",
    "    email=df\n",
    "\n",
    "    \n",
    "    \n",
    "    SPAM_PATH=os.path.join(\"datasets\",\"spam\")\n",
    "def load_data(filename,spam_path=SPAM_PATH):\n",
    "    file_path=os.path.join(spam_path,filename)\n",
    "    data=glob.glob(file_path+\"/*\")\n",
    "    mail=[open(mail,\"r\",encoding=\"utf-8_sig\",errors=\"ignore\").read() for mail in data]    \n",
    "    return mail\n",
    "\n",
    "spam_data=load_data(\"spam\")\n",
    "easy_data=load_data(\"easy_ham\")\n",
    "hard_data=load_data(\"hard_ham\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
